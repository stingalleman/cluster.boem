apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  chart:
    spec:
      chart: rook-ceph-cluster
      sourceRef:
        kind: HelmRepository
        name: rook-release
        namespace: flux-system
      version: "v1.16.6"
  interval: 24h
  values:
    # -- A list of CephFileSystem configurations to deploy
    # @default -- See [below](#ceph-file-systems)
    cephFileSystems:
      - name: ceph-filesystem
        # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
        spec:
          metadataPool:
            replicated:
              size: 3
            deviceClass: ssd
          dataPools:
            - failureDomain: host
              replicated:
                size: 3
              # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
              name: data0
              deviceClass: ssd
          metadataServer:
            activeCount: 1
            activeStandby: true
            resources:
              limits:
                memory: "4Gi"
              requests:
                cpu: "1000m"
                memory: "4Gi"
            priorityClassName: system-cluster-critical
        storageClass:
          enabled: true
          isDefault: false
          name: ceph-filesystem
          # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
          pool: data0
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: "Immediate"
          parameters:
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4
      - name: cephfs-hdd
        spec:
          metadataPool:
            replicated:
              size: 1
            deviceClass: ssd
          dataPools:
            - failureDomain: host
              replicated:
                size: 1
              name: data0
              deviceClass: hdd
          metadataServer:
            activeCount: 1
            activeStandby: true
            resources:
              limits:
                memory: "4Gi"
              requests:
                cpu: "1000m"
                memory: "4Gi"
            priorityClassName: system-cluster-critical
        storageClass:
          enabled: true
          isDefault: false
          name: cephfs-hdd
          # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
          pool: data0
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: "Immediate"
          parameters:
            # The secrets contain Ceph admin credentials.
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4

    cephBlockPools:
      - name: ceph-blockpool
        spec:
          parameters:
            min_size: "1"
          failureDomain: host
          replicated:
            size: 1
          deviceClass: ssd
          enableCrushUpdates: true
        storageClass:
          enabled: true
          name: ceph-block
          allowVolumeExpansion: true
          reclaimPolicy: Delete
          volumeBindingMode: Immediate
    # -- Namespace of the main rook operator
    operatorNamespace: rook-ceph
    # # Installs a debugging toolbox deployment
    toolbox:
      #   # -- Enable Ceph debugging pod deployment. See [toolbox](../Troubleshooting/ceph-toolbox.md)
      enabled: true
    #   # -- Toolbox image, defaults to the image used by the Ceph cluster
    #   image: #quay.io/ceph/ceph:v19.2.1
    #   # -- Toolbox tolerations
    #   tolerations: []
    #   # -- Toolbox affinity
    #   affinity: {}
    #   # -- Toolbox container security context
    #   containerSecurityContext:
    #     runAsNonRoot: true
    #     runAsUser: 2016
    #     runAsGroup: 2016
    #     capabilities:
    #       drop: ["ALL"]
    #   # -- Toolbox resources
    #   resources:
    #     limits:
    #       memory: "1Gi"
    #     requests:
    #       cpu: "100m"
    #       memory: "128Mi"
    #   # -- Set the priority class for the toolbox if desired
    #   priorityClassName:

    monitoring:
      # -- Enable Prometheus integration, will also create necessary RBAC rules to allow Operator to create ServiceMonitors.
      # Monitoring requires Prometheus to be pre-installed
      enabled: true
      rulesNamespaceOverride: monitoring
