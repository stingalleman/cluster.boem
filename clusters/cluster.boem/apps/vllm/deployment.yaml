apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm
  namespace: default
  labels:
    app: vllm
spec:
  selector:
    matchLabels:
      app: vllm
  replicas: 1
  template:
    metadata:
      labels:
        app: vllm
    spec:
      tolerations:
        - key: "dedicated"
          operator: "Equal"
          value: "gpu"
          effect: "NoSchedule"
      containers:
        - name: vllm
          image: intel/vllm:0.11.1-xpu
          command:
            - "/bin/sh"
            - "-c"
          args:
            - "vllm serve TinyLlama/TinyLlama-1.1B-Chat-v1.0"
          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: huggingface
                  key: token
          resources:
            limits:
              gpu.intel.com/i915: "1"
          ports:
            - containerPort: 8000
              name: http
          volumeMounts:
            - name: vllm
              mountPath: /root/.cache/huggingface
            # - mountPath: /dev/shm
            #   name: dshm
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 5
      volumes:
        - name: vllm
          persistentVolumeClaim:
            claimName: vllm
        # - name: dshm
        #   emptyDir:
        #     medium: Memory
        #     sizeLimit: 10Gi
      restartPolicy: Always
