# apiVersion: apps/v1
# kind: Deployment
# metadata:
#   name: vllm
#   namespace: default
#   labels:
#     app: vllm
# spec:
#   selector:
#     matchLabels:
#       app: vllm
#   replicas: 1
#   template:
#     metadata:
#       labels:
#         app: vllm
#     spec:
#       tolerations:
#         - key: "dedicated"
#           operator: "Equal"
#           value: "gpu"
#           effect: "NoSchedule"
#       containers:
#         - name: vllm
#           image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
#           command:
#             - "/bin/sh"
#             - "-c"
#           args:
#             - mkdir -p /llm/ollama && cd /llm/ollama && init-ollama && exec ./ollama serve
#           #   - "vllm serve TinyLlama/TinyLlama-1.1B-Chat-v1.0"
#           env:
#             # - name: HF_TOKEN
#             #   valueFrom:
#             #     secretKeyRef:
#             #       name: huggingface
#             #       key: token
#             - name: no_proxy
#               value: localhost,127.0.0.1
#             - name: OLLAMA_HOST
#               value: "0.0.0.0"
#             - name: DEVICE
#               value: Arc
#             - name: OLLAMA_INTEL_GPU
#               value: "true"
#             - name: OLLAMA_NUM_GPU
#               value: "999"
#             - name: ZES_ENABLE_SYSMAN
#               value: "1"
#           resources:
#             limits:
#               gpu.intel.com/i915: "1"
#           ports:
#             - containerPort: 11434
#               name: http
#           volumeMounts:
#             - name: ollama
#               mountPath: /root/.ollama
#             # - mountPath: /dev/shm
#             #   name: dshm
#           # livenessProbe:
#           #   httpGet:
#           #     path: /health
#           #     port: 8000
#           #   initialDelaySeconds: 60
#           #   periodSeconds: 10
#           # readinessProbe:
#           #   httpGet:
#           #     path: /health
#           #     port: 8000
#           #   initialDelaySeconds: 60
#           #   periodSeconds: 5
#       volumes:
#         - name: ollama
#           persistentVolumeClaim:
#             claimName: ollama
#         # - name: dshm
#         #   emptyDir:
#         #     medium: Memory
#         #     sizeLimit: 10Gi
#       restartPolicy: Always
