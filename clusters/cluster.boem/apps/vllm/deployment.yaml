apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: default
  labels:
    app: ollama
spec:
  selector:
    matchLabels:
      app: ollama
  replicas: 1
  template:
    metadata:
      labels:
        app: ollama
    spec:
      tolerations:
        - key: "dedicated"
          operator: "Equal"
          value: "gpu"
          effect: "NoSchedule"
      containers:
        - name: ollama
          image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
          command:
            - "/bin/sh"
            - "-c"
          args:
            - |
              export OLLAMA_HOST="0.0.0.0:11434"
              export OLLAMA_INTEL_GPU="true"
              export OLLAMA_NO_MMAP="1"
              export ZES_ENABLE_SYSMAN="1"
              export OLLAMA_NUM_GPU="24"
              export OLLAMA_NUM_PARALLEL="1"
              mkdir -p /llm/ollama && cd /llm/ollama &&  && exec ./ollama serve
          #   - "ollama serve TinyLlama/TinyLlama-1.1B-Chat-v1.0"
          env:
            # - name: HF_TOKEN
            #   valueFrom:
            #     secretKeyRef:
            #       name: huggingface
            #       key: token
            - name: no_proxy
              value: localhost,127.0.0.1
            - name: OLLAMA_HOST
              value: "0.0.0.0"
            - name: DEVICE
              value: Arc
            - name: OLLAMA_INTEL_GPU
              value: "true"
            - name: OLLAMA_NUM_GPU
              value: "24"
            - name: ZES_ENABLE_SYSMAN
              value: "1"
            - name: OLLAMA_NO_MMAP
              value: "1"
          resources:
            limits:
              gpu.intel.com/i915: "1"
          ports:
            - containerPort: 11434
              name: http
          volumeMounts:
            - name: ollama
              mountPath: /root/.ollama
            # - mountPath: /dev/shm
            #   name: dshm
          # livenessProbe:
          #   httpGet:
          #     path: /health
          #     port: 8000
          #   initialDelaySeconds: 60
          #   periodSeconds: 10
          # readinessProbe:
          #   httpGet:
          #     path: /health
          #     port: 8000
          #   initialDelaySeconds: 60
          #   periodSeconds: 5
      volumes:
        - name: ollama
          persistentVolumeClaim:
            claimName: ollama
        # - name: dshm
        #   emptyDir:
        #     medium: Memory
        #     sizeLimit: 10Gi
      restartPolicy: Always
